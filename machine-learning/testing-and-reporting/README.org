#+TITLE: testing-and-reporting

While developing Machine Learning systems, we want to keep evaluating the model
on a certain slice of test set where good performance is important. Every change
in model should go through a [[https://en.wikipedia.org/wiki/Regression_testing][regression run]], similar to software unit testing,
before saying that the model is good for production.

In this assignment, you will be building a plugin for a Python testing framework
that enables instance based regression tests for intent classification problem.

*  Situation
You are working on an intent classification system. While you report F1 on a
certain held-out set before deployment, people keep complaining that a few input
cases that were working before, are not working now. While the core
generalization performance of the model is something you can directly work on,
you realize that there are two other problems here:

1. These /few cases/ mentioned are more critical and so need special focus in
   evaluation.
1. A monotonic improvement in model F1 could still result in regressions in
   cases that used to work before.

To solve this, you decide to embed /these cases/ as data unit test in the regular
unit testing process. You ask the QA team to write test cases in a CSV file like
below and you want pytest to collect each line as a test item and run the test.

#+begin_src shell :exports both
head ./data/instances.csv
#+end_src

#+RESULTS:
| text                                                           | truth                 | prediction            |
| is there any discount available for early early bird discount  | early-bird-definition | early-bird-definition |
| open x                                                         | restaurant-timings    | _cancel_                |
| I want to book for two elder tomorrow                          | booking-new           | booking-new           |
| 7 o'clock                                                      | _misc_                  | _misc_                  |
| yes yes                                                        | _confirm_               | _confirm_               |
| hello                                                          | _greeting_              | _greeting_              |
| what is the price for non veg buffet on Thursday that is today | price                 | price                 |
| two people to people                                           | _misc_                  | _misc_                  |
| yes you can book for 2:30                                      | booking-new           | booking-new           |

Note that in reality the output (~prediction~) will come in real time from a
function, but for the purpose of this exercise we will keep that also in the
same data file.

** Problem
You read up documentation of [[https://docs.pytest.org/en/latest/][pytest]] and decide that [[https://docs.pytest.org/en/latest/how-to/writing_plugins.html][writing a plugin]] for pytest
is a good way to move forward. The plugin is supposed to do the following:

+ Pick up files with names like ~test_*.csv~ under ~./tests/~ as source of test
  cases.
+ Collect each instance in the CSVs as single test items.
+ Run assertions on truth vs prediction matching.
+ /Optionally/ produce a [[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html][regular classification report]] as an artifact of test run.

* Output
When done, provide us access to a private Github repository with your the code.
The code should have the following:

1. Your plugin module as per pytest conventions in the code template provided
   alongside.
3. README with instructions on setting up and using this new testing approach.

* Solution
Made a Pytest script which takes in the directory containing the ~test_*.csv~ files and gives output all the failure cases with a classification report for all the files. Did not understand how to make a plugin module for Pytest and what were the expectations.

* Running the script
1. Make sure the ~test_report.py~ script is in the same directory as the ~/tests~ folder.
2. Run the requirements.txt file.
3. Run the script as:
   ~$pytest -s~
4. Voila! You have all the failure cases with the classification reports.
